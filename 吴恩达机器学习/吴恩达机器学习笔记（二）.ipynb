{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 吴恩达机器学习笔记（二）\n",
    "\n",
    "\n",
    "由于最近忙着找工作，更新进程一度被中断，两周前已经学完的课程，现在才有时间进行总结。\n",
    "\n",
    "本部分主要对应 [Coursera](coursera.org) 吴恩达机器学习的4~6周，神经网络部分。\n",
    "\n",
    "部分内容参考 [一文弄懂神经网络中的反向传播法——BackPropagation](https://www.cnblogs.com/charlotte77/p/5629865.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、神经网络\n",
    "\n",
    "动机：我们已经有线性回归、多项式回归和逻辑回归了，为什么还要研究神经网络？\n",
    "\n",
    "神经网络算法可以解决复杂的非线性问题。\n",
    "\n",
    "复杂的机器学习问题，涉及的项往往多于两项，这时候用逻辑回归或多项式回归，往往会有过拟合的问题，另外运算量也过大。\n",
    "\n",
    "神经网络是一种很古老的算法，他最初产生的目的是制造能模拟大脑的机器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、神经网络的结构\n",
    "\n",
    "神经网络的结构如下，分为输入层、隐含层和输出层。\n",
    "\n",
    "![神经网络结构](神经网络结构.png)\n",
    "\n",
    "其中，$x_0$, $a_0$ 为偏置，常设为1。\n",
    "\n",
    "对于多分类的情况，神经网络结构如下：\n",
    "![神经网络多分类结构](神经网络多分类结构.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、一个神经网络的完整传播过程\n",
    "\n",
    "以如下结构的神经网络为例，\n",
    "\n",
    "<img style=\"float: center;\" src=\"神经网络反向传播.jpg\" width=\"50%\">\n",
    "\n",
    "\n",
    "这个网络的输入为 X1、X2，X0=1 为输入层偏置，输出为h1、h2，隐含层为 a1、a2，a0=1 为隐含层偏置，激活函数为sigmoid函数。\n",
    "\n",
    "首先，对这个网络赋初值，\n",
    "\n",
    "输入数据：\n",
    "\n",
    "$$ x=[x_0, x_1, x_2]^T=[1, 0.05, 0.1]^T $$\n",
    "\n",
    "输出数据：\n",
    "\n",
    "$$ y=[y_1, y_2]^T=[0.01, 0.99]^T $$\n",
    "\n",
    "初始权值：\n",
    "\n",
    "$$ \\theta^{(1)} = \\left(\n",
    "  \\begin{array}{ccc}\n",
    "    \\theta_{01}^{(1)} & \\theta_{02}^{(1)}\\\\ \n",
    "    \\theta_{11}^{(1)} & \\theta_{12}^{(1)}\\\\  \n",
    "    \\theta_{21}^{(1)} & \\theta_{22}^{(1)}\\\\ \n",
    "  \\end{array}\n",
    "\\right) = \\left(\n",
    "  \\begin{array}{ccc}\n",
    "    0.35 & 0.35\\\\\n",
    "    0.15 & 0.25\\\\  \n",
    "    0.2 & 0.3\\\\ \n",
    "  \\end{array}\n",
    "\\right) $$\n",
    "\n",
    "$$ \\theta^{(2)} = \\left(\n",
    "  \\begin{array}{ccc}\n",
    "    \\theta_{01}^{(2)} & \\theta_{02}^{(2)}\\\\ \n",
    "    \\theta_{11}^{(2)} & \\theta_{12}^{(2)}\\\\  \n",
    "    \\theta_{21}^{(2)} & \\theta_{22}^{(2)}\\\\ \n",
    "  \\end{array}\n",
    "\\right) = \\left(\n",
    "  \\begin{array}{ccc}\n",
    "   0.6 & 0.6\\\\\n",
    "    0.4 & 0.5\\\\  \n",
    "    0.45 & 0.55\\\\ \n",
    "  \\end{array}\n",
    "\\right) $$\n",
    "\n",
    "\n",
    "### 3.1 前向传播\n",
    "\n",
    "(1) 输入层 --> 隐含层\n",
    "\n",
    "$$z_1^{(1)} = x_0 * \\theta_{01}^{(1)} + x_1 * \\theta_{11}^{(1)} + x_2 * \\theta_{21}^{(1)}=0.35*1+0.15*0.05+0.2*0.1=0.3775$$\n",
    "$$z_2^{(1)} = x_0 * \\theta_{02}^{(1)} + x_1 * \\theta_{12}^{(1)} + x_2 * \\theta_{22}^{(1)}=0.35*1+0.05*0.25+0.3*0.1=0.3925$$\n",
    "\n",
    "$$a_1 = g(z_1^{(1)})=0.59327$$\n",
    "$$a_2 = g(z_2^{(1)})=0.59688$$\n",
    "\n",
    "其中，$g(x) = sigmoid(x)=1/(1+\\exp(-x))$.\n",
    "\n",
    "(2) 隐含层 --> 输出层\n",
    "\n",
    "$$z_1^{(2)} = a_0 * \\theta_{01}^{(2)} + a_1 * \\theta_{11}^{(2)} + a_2 * \\theta_{21}^{(2)}$$\n",
    "$$z_2^{(2)} = a_0 * \\theta_{02}^{(2)} + a_1 * \\theta_{12}^{(2)} + a_2 * \\theta_{22}^{(2)}$$\n",
    "\n",
    "$$h_1 = g(z_1^{(2)})=0.75137$$\n",
    "$$h_2 = g(z_2^{(2)})=0.77293$$\n",
    "\n",
    "将以上**前向传播**的过程写成矩阵形式：\n",
    "\n",
    "$$(a_1, a_2)^T = g(\\theta^{(1)T}x)$$\n",
    "$$ a = (a_0, a_1, a_2)^T$$\n",
    "$$(h_1, h_2)^T = g(\\theta^{(2)T}a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 反向传播\n",
    "\n",
    "(1) 计算总误差\n",
    "\n",
    "定义代价函数，\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "J(\\theta) &=& -\\frac{1}{m}[\\sum_{i=1}^m\\sum_{k=1}^{K}( y_k^{(i)}\\log(h_k^{(i)})+(1-y_k^{(i)})\\log(1-h_k^{(i)})]+\\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}(\\theta_{ji}^{(l)})^2\\\\ \n",
    "&=& -\\frac{1}{m}\\sum_{i=1}^{m}[y^T\\log(h)+(1-y)^T\\log(1-h))]+\\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}(\\theta_{ji}^{(l)})^2\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "其中，$L$为总层数，$s_l$第$l$层的节点数量，$m$为样本数，$k$为输出节点数。\n",
    "\n",
    "在本例中，$L=2$，$s_1=2$，$s_2=2$，$m=1$，\n",
    "\n",
    "$$J(\\theta)=-[y^T\\log(h)+(1-y)^T\\log(1-h))]=1.6505$$\n",
    "\n",
    "（2）输出层 --> 隐含层的权值更新\n",
    "\n",
    "以权值$\\theta_{11}^{(2)}$为例，如果我们想知道$\\theta_{11}^{(2)}$对整体误差产生了多少影响，可以用整体误差对$\\theta_{11}^{(2)}$求偏导得到：\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_{11}^{(2)}}=\\frac{\\partial J(\\theta)}{\\partial h_1}*\\frac{\\partial h_1}{\\partial z_1^{(2)}}*\\frac{\\partial z_1^{(2)}}{\\partial \\theta_{11}^{(2)}}$$\n",
    "\n",
    "下图直观的反映了误差是怎样反向传播的：\n",
    "<img style=\"float: center;\" src=\"反向传播细节.jpg\" width=\"25%\">\n",
    "\n",
    "现在分别计算每个式子的值：\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial h_1}=\\frac{h_1-y_1}{h_1*(1-h_1)}=3.9684$$\n",
    "\n",
    "$$\\frac{\\partial h_1}{\\partial z_1^{(2)}}=h_1*(1-h_1)=0.1868$$\n",
    "\n",
    "$$\\frac{\\partial z_1^{(2)}}{\\partial \\theta_{11}^{(2)}}=a_1=0.59327$$\n",
    "\n",
    "将3式相乘：\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_{11}^{(2)}}=0.4398$$\n",
    "\n",
    "\n",
    "这样我们就计算出对$\\theta_{11}^{(2)}$的偏导数。\n",
    "\n",
    "回过头再看看上面的公式，我们发现：\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_{11}^{(2)}}=\\frac{h_1-y_1}{h_1*(1-h_1)}*h_1*(1-h_1)*a_1$$\n",
    "\n",
    "为了方便，用$\\delta_{h1},\\delta_{h2}$来表示输出层误差：\n",
    "\n",
    "$$\\delta_{h1}=\\frac{\\partial J(\\theta)}{\\partial z_1}=\\frac{\\partial J(\\theta)}{\\partial h_1}*\\frac{\\partial h_1}{\\partial z_1^{(2)}}=h_1-y_1$$\n",
    "$$\\delta_{h2}=\\frac{\\partial J(\\theta)}{\\partial z_2}=\\frac{\\partial J(\\theta)}{\\partial h_2}*\\frac{\\partial h_2}{\\partial z_2^{(2)}}=h_2-y_2$$\n",
    "\n",
    "因此，整体误差对$\\theta_{11}^{(2)}$的偏导可以写成：\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_{11}^{(2)}}=\\delta_{h1}*a_1$$\n",
    "\n",
    "最后更新$\\theta_{11}^{(2)}$的值：\n",
    "$$\\theta_{11}^{(2)+}=\\theta_{11}^{(2)}-\\eta * \\delta_{h1}*a_1=0.1801$$\n",
    "其中，$\\eta$为学习率，这里取0.5。\n",
    "\n",
    "同理，可以更新$\\theta_{12}^{(2)},\\theta_{21}^{(2)},\\theta_{22}^{(2)}$：\n",
    "\n",
    "$$\\theta_{12}^{(2)+}=\\theta_{12}^{(2)} - \\eta * \\delta_{h2}*a_1=0.5643,$$\n",
    "$$\\theta_{21}^{(2)+}=\\theta_{21}^{(2)} - \\eta * \\delta_{h1}*a_2=0.2287,$$\n",
    "$$\\theta_{22}^{(2)+}=\\theta_{22}^{(2)} - \\eta * \\delta_{h2}*a_2=0.6148.$$\n",
    "\n",
    "----------\n",
    "\n",
    "将以上过程写成矩阵形式：\n",
    "$$\\delta_{h}=\\frac{\\partial J(\\theta)}{\\partial z}=\\frac{\\partial J(\\theta)}{\\partial h}*\\frac{\\partial h}{\\partial z^{(2)}}=h-y$$\n",
    "\n",
    "由于不更新偏置的权值，这里忽略对$\\theta_{01}^{(2)}$和$\\theta_{02}^{(2)}$的偏导，\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta^{(2)}}=\\left(\n",
    "\\begin{array}{cc}\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_{11}^{(2)}} & \\frac{\\partial J(\\theta)}{\\partial \\theta_{12}^{(2)}}\\\\\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_{21}^{(2)}} & \\frac{\\partial J(\\theta)}{\\partial \\theta_{22}^{(2)}}\n",
    "\\end{array}\\right)\n",
    "=\\left( \\begin{array}{cc}\n",
    "\\delta_{h1}*a_1 & \\delta_{h2}*a_1 \\\\\n",
    "\\delta_{h1}*a_2 & \\delta_{h2}*a_2\n",
    "\\end{array} \\right)\n",
    "=(a_1, a_2)^T \\cdot \\delta_h^T$$\n",
    "$$\\theta^{(2)+} = \\theta^{(2)}[1:] - \\eta \\frac{\\partial J(\\theta)}{\\partial \\theta^{(2)}}$$\n",
    "(注，这里用了numpy中的语法，表示更新$\\theta^{(2)}$第一行以后的所有行。)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（3）隐含层-->输入层权值更新\n",
    "\n",
    "采取以上类似的步骤，但值得注意的是，在计算总误差对$\\theta^{(2)}_{11}$的偏导时，是从$h_1->z_1^{(2)}->\\theta^{(2)}_{11}$，但是在隐含层-->输入层（或者隐含层-->隐含层）的权值更新时，是$a_1->z_1^{(1)}->\\theta^{(1)}_{11}$，而$a_1$会接受两个地方传来的误差，所以这两个地方都要计算。\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta^{(1)}_{11}}=(\\frac{\\partial J(\\theta)}{\\partial h_1} * \\frac{\\partial h_1}{\\partial z_1^{(2)}} * \\frac{\\partial z_1^{(2)}}{\\partial a_1} + \\frac{\\partial J(\\theta)}{\\partial h_2} * \\frac{\\partial h_2}{\\partial z_2^{(2)}} * \\frac{\\partial z_2^{(2)}}{\\partial a_1}) * \\frac{\\partial a_1}{\\partial z_1^{(1)}} * \\frac{\\partial z^{(1)}_1}{\\partial \\theta^{(1)}_{11}}$$\n",
    "\n",
    "同样的用$\\delta_{a1},\\delta_{a2}$代表隐含层误差，\n",
    "$$\\begin{eqnarray*} \\delta_{a1} &=& (\\frac{\\partial J(\\theta)}{\\partial h_1} * \\frac{\\partial h_1}{\\partial z_1^{(2)}} * \\frac{\\partial z_1^{(2)}}{\\partial a_1} + \\frac{\\partial J(\\theta)}{\\partial h_2} * \\frac{\\partial h_2}{\\partial z_2^{(2)}} * \\frac{\\partial z_2^{(2)}}{\\partial a_1}) * \\frac{\\partial a_1}{\\partial z_1^{(1)}}\\\\\n",
    "&=& ((h_1-y_1)h_1(1-h_1)\\theta_{11}^{(2)}+(h_2-y_2)h_2\\theta^{(2)}_{12}) * a_1(1-a_1)\\\\\n",
    "&=& (\\delta_{h1}\\theta_{11}^{(2)}+\\delta_{h2}\\theta_{12}^{(2)})a_1(1-a_1)\\\\\n",
    "&=& 0.04537\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "$$\\delta_{a2} = (\\delta_{h1}\\theta_{21}^{(2)}+\\delta_{h2}\\theta_{22}^{(2)})a_2(1-a_2)=0.05155$$\n",
    "\n",
    "输入层与隐含层之间的权值更新，\n",
    "$$\\theta^{(1)+}_{11} = \\theta^{(1)}_{11} - \\eta \\cdot \\delta_{a1} \\cdot x_1=0.1489$$\n",
    "$$\\theta^{(1)+}_{12} = \\theta^{(1)}_{12} - \\eta \\cdot \\delta_{a2} \\cdot x_1=0.2487$$\n",
    "$$\\theta^{(1)+}_{21} = \\theta^{(1)}_{21} - \\eta \\cdot \\delta_{a1} \\cdot x_2=0.1977$$\n",
    "$$\\theta^{(1)+}_{22} = \\theta^{(1)}_{22} - \\eta \\cdot \\delta_{a2} \\cdot x_2=0.2974$$\n",
    "\n",
    "-----------\n",
    "\n",
    "将以上过程写成矩阵形式，\n",
    "$$\\begin{eqnarray*}\n",
    "\\delta_{a} &=& \\left(\n",
    "\\begin{array}{cc}\n",
    "\\theta^{(2)}_{11} & \\theta_{12}^{(2)}\\\\\n",
    "\\theta_{21}^{(2)} & \\theta_{22}^{(2)}\n",
    "\\end{array}\\right)\n",
    "\\left( \\begin{array}{c}\n",
    "\\delta_{h1}\\\\\n",
    "\\delta_{h2}\n",
    "\\end{array}\\right)\n",
    "*\\left( \\begin{array}{c}\n",
    "a_1\\\\\n",
    "a_2\n",
    "\\end{array}\\right)\n",
    "*\\left( \\begin{array}{c}\n",
    "1-a_1\\\\\n",
    "1-a_2\n",
    "\\end{array}\\right)\\\\\n",
    "&=&\\theta^{(2)}[1:] \\cdot \\delta_{h} * a[1:]*(1-a[1:])\n",
    "\\end{eqnarray*}$$\n",
    "(注，这里的`*`为numpy中的`*`，代表对应元素相乘，Matlab中应为`.*`。)\n",
    "$$\\theta^{(1)+} = \\theta^{(1)}[1:] - \\eta * \\delta_a^T * x[1:]$$\n",
    "\n",
    "<font color=Crimson>注意，计算出所有权值的更新值后再统一更新，</font>\n",
    "\n",
    "$$\\theta^{(1)}[1:] = \\theta^{(1)+}$$\n",
    "$$\\theta^{(2)}[1:] = \\theta^{(2)+}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Python实现\n",
    "\n",
    "下面将用Python对上述的例子实现，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta1= [[0.35       0.35      ]\n",
      " [0.14886582 0.24871137]\n",
      " [0.19773165 0.29742273]]\n",
      "theta2= [[0.6        0.6       ]\n",
      " [0.18008518 0.56439101]\n",
      " [0.22874539 0.6147833 ]]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "x = np.array([1,0.05,0.1]).reshape(-1,1) # 列向量\n",
    "y = np.array([0.01, 0.99]).reshape(-1,1) # 列向量\n",
    "theta1 = np.array([[0.35,0.15,0.2],[0.35,0.25,0.3]]).T\n",
    "theta2 = np.array([[0.6,0.4,0.45],[0.6,0.5,0.55]]).T\n",
    "\n",
    "# 前向传播\n",
    "# 输入层-->隐含层\n",
    "z1 = np.dot(theta1.T, x)\n",
    "a = sigmoid(z1)\n",
    "a1, a2 = a[0], a[1]\n",
    "a = np.insert(a, 0, values = 1, axis = 0) # 在a[0]处插入1\n",
    "# 隐含层--> 输出层\n",
    "z2 = np.dot(theta2.T, a)\n",
    "h = sigmoid(z2)\n",
    "h1, h2 = h[0], h[1]\n",
    "\n",
    "# 反向传播\n",
    "# 代价函数（总误差）\n",
    "J = -(np.dot(y.T, np.log(h))+np.dot((1-y).T, np.log(1-h)))\n",
    "eta = 0.5\n",
    "# 输出层-->隐含层\n",
    "delta_h = h-y\n",
    "theta2_tmp = theta2[1:] - eta * delta_h.T * a[1:]\n",
    "# 隐含层-->输入层\n",
    "delta_a = theta2[1:] @ delta_h * a[1:] * (1-a[1:]) #numpy中@表示矩阵乘法，等同于np.dot()，*为对应元素相乘\n",
    "theta1_tmp = theta1[1:] - eta * delta_a.T * x[1:]\n",
    "# 最后更新权值\n",
    "theta1[1:] = theta1_tmp\n",
    "theta2[1:] = theta2_tmp\n",
    "\n",
    "print('theta1=', theta1)\n",
    "print('theta2=', theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、梯度检测\n",
    "\n",
    "在之前几节里，我们讨论了如何进行前向传播，以及后向传播，从而计算导数。但，后向传播有很多细节，这些细节有点复杂，有一个不幸的消息是，它们有很多细节会导致一些BUG，如果你用梯度下降来计算，你会发现表面上它可以工作，实际上，J 虽然每次迭代都在下降，但是可能，仍然你的代码有很多BUG。所以，表面上关于theta的函数J在减小，但是你可能最后得到的结果，实际上有很大的误差。你这时候可能知道，有一些小的BUG导致这种不好的算法性能表现。所以，怎么办呢？有一个想法叫梯度检验 Gradient Checking，它能减少这种错误的概率。就我个人而言，每次我使用后向传播，我都会用这种方法，即使是其他比较复杂的模型，我都会做这种检查，如果你这么做，你会对你的模型更有自信，这样，你会更加确信的模型是100%正确的。从我看到的情况，这种方法，很大程度可以减少错误的可能性。\n",
    "\n",
    "梯度检测的原理很简单，核心思想就是下面这个数学近似，\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_{i}} \\approx \\frac{J(\\theta_1,\\theta_2,\\ldots,\\theta_i + \\varepsilon, \\ldots,\\theta_n)-J(\\theta_1,\\theta_2,\\ldots,\\theta_i - \\varepsilon, \\ldots,\\theta_n)}{2 \\varepsilon}$$\n",
    "\n",
    "具体的如何操作呢？我们分别计算出用反向传播得到的梯度和上面的公式得出的梯度，将两者对比，如果相等或者非常相近，那么可以确定反向传播的实现是正确的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta1= [[0.35       0.35      ]\n",
      " [0.14886582 0.24871137]\n",
      " [0.19773165 0.29742273]]\n",
      "theta2= [[0.6        0.6       ]\n",
      " [0.18008518 0.56439101]\n",
      " [0.22874539 0.6147833 ]]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y, theta1, theta2, eta):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.theta1 = theta1\n",
    "        self.theta2 = theta2\n",
    "        self.eta = eta\n",
    "        self.a = self.hiddenLayer(self.theta1)\n",
    "        self.h = self.output(self.theta1, self.theta2)\n",
    "        self.delta_h = self.h-self.y\n",
    "        self.delta_a = self.theta2[1:] @ self.delta_h * self.a[1:] * (1-self.a[1:])\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def hiddenLayer(self, theta1):\n",
    "        a = self.sigmoid(theta1.T @ self.x)\n",
    "        a = np.insert(a, 0, values = 1, axis = 0)\n",
    "        return a\n",
    "    \n",
    "    def output(self, theta1, theta2):\n",
    "        h = self.sigmoid(theta2.T @ self.hiddenLayer(theta1))\n",
    "        return h\n",
    "    \n",
    "    def costFunJ(self, theta1, theta2):\n",
    "        h = self.output(theta1, theta2)\n",
    "        return -(self.y.T @ np.log(h) + (1-self.y).T @ np.log(1-h))\n",
    "    \n",
    "    def updateTheta(self):\n",
    "        self.theta1[1:] = self.theta1[1:] - self.eta * self.delta_a.T * self.x[1:]\n",
    "        self.theta2[1:] = self.theta2[1:] - self.eta * self.delta_h.T * self.a[1:]\n",
    "        return\n",
    "    \n",
    "    @property\n",
    "    def gradChick(self):\n",
    "        dVec1 = self.delta_a.T * self.x[1:]\n",
    "        dVec1 = dVec1.reshape(-1,1)\n",
    "        dVec2 = self.delta_h.T * self.a[1:]\n",
    "        dVec2 = dVec2.reshape(-1,1)\n",
    "        dVec = np.concatenate([dVec1, dVec2])\n",
    "        epsilon = 0.0001\n",
    "        grad1 = np.zeros(theta1[1:].shape)\n",
    "        grad2 = np.zeros(theta2[1:].shape)\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                thetaPlus1 = self.theta1.copy()\n",
    "                thetaPlus1[i+1,j] =  thetaPlus1[i+1,j] + epsilon\n",
    "                thetaMinus1 = self.theta1.copy()\n",
    "                thetaMinus1[i+1,j] =  thetaMinus1[i+1,j] - epsilon         \n",
    "                grad1[i][j] = (self.costFunJ(thetaPlus1,theta2)-self.costFunJ(thetaMinus1, theta2))/(2*epsilon)\n",
    "                thetaPlus2 = self.theta2.copy()\n",
    "                thetaPlus2[i+1,j] =  thetaPlus2[i+1,j] + epsilon\n",
    "                thetaMinus2 = self.theta2.copy()\n",
    "                thetaMinus2[i+1,j] =  thetaMinus2[i+1,j] - epsilon         \n",
    "                grad2[i][j] = (self.costFunJ(theta1,thetaPlus2)-self.costFunJ(theta1,thetaMinus2))/(2*epsilon)\n",
    "        grad1 = grad1.reshape(-1,1)\n",
    "        grad2 = grad2.reshape(-1,1)\n",
    "        grad = np.concatenate([grad1, grad2])\n",
    "        if np.sum(np.abs(dVec - grad)) <= 0.0001:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "if __name__=='__main__':\n",
    "    x = np.array([1,0.05,0.1]).reshape(-1,1) # 列向量\n",
    "    y = np.array([0.01, 0.99]).reshape(-1,1) # 列向量\n",
    "    theta1 = np.array([[0.35,0.15,0.2],[0.35,0.25,0.3]]).T\n",
    "    theta2 = np.array([[0.6,0.4,0.45],[0.6,0.5,0.55]]).T\n",
    "    eta = 0.5\n",
    "    NN = NeuralNetwork(x, y, theta1, theta2, eta)\n",
    "    if NN.gradChick:\n",
    "        NN.updateTheta()\n",
    "        print('theta1=', NN.theta1)\n",
    "        print('theta2=', NN.theta2)\n",
    "    else:\n",
    "        print('请检查反向传播的实现是否正确')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、随机初始化\n",
    "\n",
    "在逻辑回归中，我们经常将权值的初始值定为0，在神经网络中是不能这样的。因为sigmoid(0)=0.5，如果所有的权值都为零，那么所有节点的输出都为0.5，0.5乘以0又为0，这样无论更新多少次，最终的权值依然还是0.所以，在神经网络中，我们需要将权值初始化为一组不全为0的值。常用的方法是随机初试化。例如，希望所有的权值$(3*2)$初始化在$[-\\epsilon,\\epsilon]$这个区间内，可以这样做，\n",
    "$$\\theta = rand(3,2) * 2\\epsilon - \\epsilon$$\n",
    "在`numpy`中可以很方便的使用`np.random.random()`实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.88627707, -1.4848173 ],\n",
       "       [-1.87038561, -0.4826387 ],\n",
       "       [-1.90311163, -1.30681426]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random((3,2))*4-2 # epsilon=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6、评估假设\n",
    "\n",
    "前面已经介绍了神经网络算法的基本步骤，通过这些步骤基本可以保证正确的实现一个神经网络模型。但是当我们在做一个实际项目的时候，可能经常会遇到这样的问题，我们训练出来的模型并不能对新数据很好的预测，或者说预测效果达不到预期，在碰到这种情况的时候,大多数人都知道从以下几个角度去尝试：\n",
    "* 得到更多的训练样本\n",
    "* 尝试更少的特征\n",
    "* 增加特征\n",
    "* 增加多项式特征\n",
    "* 减小$\\lambda$\n",
    "* 增加$\\lambda$\n",
    "但是造成模型预测效果不好的原因这么多，如何能够找到问题，并且对症下药，优化我们的模型呢？\n",
    "\n",
    "下面将介绍一些机器学习诊断方法，去帮助找到问题，把时间花在刀刃上。\n",
    "\n",
    "（1）测试误差\n",
    "\n",
    "测试误差是一种简单的评价测试集误差的指标。首先将所有的数据按7:3，分为训练集和测试集。对于线性回归来说，通常代价函数定义为，\n",
    "$$J(\\theta) = \\frac{1}{2m}(h_{\\theta}(x)-y)^T(h_{\\theta}(x)-y)$$\n",
    "将以上定义用于测试集，即可得到测试误差。\n",
    "\n",
    "对于逻辑回归也可以用相同的思想，定义测试误差。另外还有一种更易于理解的定义，统计测试集误分类的次数，然后除以测试样本数，得到误分类率。\n",
    "\n",
    "（2）模型选择\n",
    "\n",
    "当我们拿到一组数据的时候，最先考虑的可能就是模型选择的问题了。对于以下模型，到底应该用哪一个呢？\n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1x$$\n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1x + \\theta_2x^2$$\n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1x + \\theta_2x^2 + \\theta_3x^3$$\n",
    "$$\\cdots$$\n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1x + \\theta_2x^2+\\cdots+\\theta_{10}x^{10}$$\n",
    "\n",
    "对于这个问题我们可以将数据分为3部分，分别是训练集、交叉验证集、测试集，比例为60%,20%,20%，我们在训练集上得到$\\theta$，在交叉验证集上得到模型，最后用测试集验证泛化能力。\n",
    "\n",
    "评价指标依然是训练误差、交叉验证误差(Cross Validation)和测试误差，\n",
    "\n",
    "$$J_{treain}(\\theta) = \\frac{1}{2m}(h_{\\theta}(x)-y)^T(h_{\\theta}(x)-y)$$\n",
    "$$J_{cv}(\\theta) = \\frac{1}{2m_{cv}}(h_{\\theta}(x_{cv})-y_{cv})^T(h_{\\theta}(x_{cv})-y_{cv})$$\n",
    "$$J_{test}(\\theta) = \\frac{1}{2m_{test}}(h_{\\theta}(x_{test})-y_{test})^T(h_{\\theta}(x_{test})-y_{test})$$\n",
    "\n",
    "为什么要有交叉验证集，而不是直接用测试集决定最终的模型呢呢？\n",
    "\n",
    "因为用测试误差最小化得到了我们的多项式次数d，这也就是说，我们选择了一个能够最好地拟合测试集的参数d，因此我再用测试集来评价我的假设就显得不公平了。我们其实是更关心模型对新样本的拟合效果的。具体来讲，我们做的实际上是用测试集来拟合参数d，通过用测试集来拟合这个参数，同样也意味着，这并不能较为公平地预测出假设函数的在遇到新样本时的表现。为了解决这一问题，在模型选择中，如果我们想要评价某个假设，我们通常采用以下的方法，给定某个数据集，我们要将其分为三段，第一部分还是叫训练集，第二部分我把它叫做交叉验证集（cross validation set），第三部分为测试集。我们用训练集得到各个模型，然后选择交叉验证集误差最小的那个假设，最后用测试集来评价模型的表现。\n",
    "\n",
    "（3）诊断bias和variance\n",
    "\n",
    "如果一个模型的表现不好，多半是以下两种情况，要么是bias（偏差）过大，要么是variance（方差）过大，换句话说，要么是欠拟合，要么是过拟合。那么如何判断当前是哪种情况呢？一图以蔽之。\n",
    "\n",
    "<img style=\"float: center;\" src=\"诊断bias或variance.png\" width=\"100%\">\n",
    "\n",
    "简单来说，就是训练误差很大，交叉验证误差也很大，则为欠拟合；训练误差很小，交叉验证误差很大，则为过拟合。\n",
    "\n",
    "（4）正则化对bias和variance的影响\n",
    "\n",
    "我们知道，正则化在一定程度上可以解决过拟合问题，但是$\\lambda$过大也会导致欠拟合。相似的如果我们把正则化参数$\\lambda$作为横坐标，训练误差和交叉验证误差作为纵坐标，我们可以得到下面的图像。\n",
    "\n",
    "<img style=\"float: center;\" src=\"正则化对拟合效果的影响.png\" width=\"100%\">\n",
    "\n",
    "在我们选择正则化参数时，可以选择一组$\\lambda$的值，例如$\\lambda = 0, 0.01, 0.02, 0.04,0.08,\\ldots, 10$，以后一项是前一项两倍的规律递增，画出这样的曲线，自动或手动的选择两种误差都比较小时的$\\lambda$。\n",
    "\n",
    "（5）学习曲线（learning curves）\n",
    "\n",
    "学习曲线的横坐标是样本的特征数，纵坐标是训练误差和交叉验证误差。通过画出随着样本数量的增加，模型的误差曲线，我们也可以判断当前模型是处于过拟合还是欠拟合。\n",
    "\n",
    "一个拟合效果比较好的模型，它的学习曲线是这样的，\n",
    "<img style=\"float: center;\" src=\"学习曲线（效果好）.png\" width=\"100%\">\n",
    "\n",
    "欠拟合的效果是这样的，训练误差和交叉验证误差都比较大，而且，随着样本的增多，误差并不会减小。\n",
    "\n",
    "<img style=\"float: center;\" src=\"学习曲线（欠拟合）.png\" width=\"100%\">\n",
    "\n",
    "过拟合是这样的，训练误差虽然增加但一直很小，交叉验证误差虽然减小，但一直很大，两者有个很宽的间隔。\n",
    "<img style=\"float: center;\" src=\"学习曲线（过拟合）.png\" width=\"100%\">\n",
    "\n",
    "（6）解决问题\n",
    "\n",
    "上面提到的6种方法分别对应在什么时候使用呢？\n",
    "\n",
    "* 得到更多的训练样本 --> 过拟合\n",
    "* 尝试更少的特征 --> 过拟合\n",
    "* 增加特征 --> 欠拟合\n",
    "* 增加多项式特征 --> 欠拟合 \n",
    "* 减小$\\lambda$ --> 欠拟合\n",
    "* 增加$\\lambda$ --> 过拟合\n",
    "\n",
    "对于神经网络如何判断过拟合还是欠拟合呢？\n",
    "\n",
    "一般来说，神经网络的节点少，隐含层少，对应的模型过于简单，可能会出现欠拟合；神经网络的节点多或隐含层多则拟合效果好，但是容易出现过拟合。通常过拟合问题可以通过使用正则化来修正。如果经常使用神经网络，你会发现，一般大型的神经网络拟合效果比小型的更好，但是主要可能出现的一个问题就是，计算量相对较大。\n",
    "\n",
    "如果你想选择神经网络的层数，也可以像上面提到的方法那样，将数据集分成3个部分，分别在神经网络为1层、2层、3层等等的时候，画出训练误差和交叉验证误差曲线，选择交叉验证误差最小的那一个。\n",
    "\n",
    "（7）不对称性数据\n",
    "\n",
    "什么是不对称数据？\n",
    "\n",
    "例如一个关于癌症诊断的数据集，其中正样本（患病）的只有0.5%，负样本（未患病）的有99.5%，类似这种，正负样本的比例相差非常悬殊的叫做不对称数据。假设对于这个数据集，你建立了一个机器学习的模型去对患者进行诊断，你发现，你的测试误差只有1%，而如果什么都不做，只是对所有的样本都输出未患病，这样的测试误差将降到0.5%，显然这时候用测试误差去衡量学习效果是不对的。\n",
    "\n",
    "那么对于不对称数据应该采用什么指标去评价学习效果呢？\n",
    "\n",
    "下面将引入查准率和召回率，\n",
    "\n",
    "<table border=\"1\" align=\"center\">\n",
    "    <tr>\n",
    "        <td colspan = \"2\"> </td> \n",
    "        <td colspan = \"2\"><div align=\"center\">Actual class</div></td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan = \"2\"> </td> \n",
    "        <td>1</td> \n",
    "        <td>0</td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"2\">Predicted class</td>\n",
    "        <td>1</td>\n",
    "        <td>True positive</td>\n",
    "        <td>False positive</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>False negative</td>\n",
    "        <td>True negative</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "$$ \\text{查准率(Precision)} = \\frac{\\text{True positives}}{\\text{Predicted positive}} = \\frac{\\text{True positive}}{\\text{True positives + False positives}}$$\n",
    "\n",
    "$$ \\text{召回率(Recall)} = \\frac{\\text{True positives}}{\\text{Actual positive}} = \\frac{\\text{True positive}}{\\text{True positives + False negative}}$$\n",
    "\n",
    "我们希望模型具有比较高的查准率和召回率。对于以上癌症的例子，如果总是输出$y=0$，虽然有比较高的$\\text{Precision}$，但是$\\text{Recall}=0$，显然，这不是一个好的模型。\n",
    "\n",
    "对于一般的逻辑回归，我们对于分类的阈值是$0.5$，当输出$h \\geq 0.5$，预测样本的类别为1，否则预测样本类别为0。还是以癌症分类为例，假如我们希望非常确定的时候才给出患病的判决（毕竟告诉对方得了癌症是一个不是一个好消息），这时候，我们将分类的阈值设置为$0.9$，这将是一个高查准率（Predicted positive 比较小）、低召回率的模型。现在考虑另外一种情况，假如我们希望不放过任何患病的可能（如果一个患了病的，我们告诉他没患病，这样会耽误他的治疗），即我们希望避免假阴性，这时候，我们将分类阈值设置为$0.3$，这将是一个高召回率（Predicted positive 比较大）、低查准率的模型。不同的问题，根据实际需要，可以选择不同的阈值。这里又产生了一个有趣的问题，在实际选择阈值的时候，到底是0.5的$Precision$和$0.4$的$Recall$好，还是$0.7$的$Precision$和$0.1$的$Recall$好呢？这里可以引入一个评价指标，$F_1$ Score(F Score)，用P表示Precision，用R表示Recall，\n",
    "$$F_1\\text{ Score} = 2\\frac{PR}{P+R}$$\n",
    "\n",
    "（8）机器学习的数据\n",
    "\n",
    "对于机器学习来说，数据很重要，但是也不要盲目的去收集大量的数据。诚然，在有些时候大量的数据是能很好的提高机器学习的效果，但是这种情况往往出现在，这些条件对你的问题都成立，并且能够收集到很多数据的基础上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7、机器学习系统的设计\n",
    "\n",
    "下面以一个垃圾邮件分类器为例，介绍一个机器学习系统的设计过程。\n",
    "\n",
    "（1）选择特征向量\n",
    "\n",
    "选择一些单词按照某种顺序（例如字典序）排列，作为邮件的特征向量，如果这个单词出现了，该位置的值为1，如果没出现，该位置的值为0.\n",
    "<img style=\"float: center;\" src=\"邮件分类特征向量.png\" width=\"100%\">\n",
    "\n",
    "（2）快速建立一个简单的模型\n",
    "\n",
    "在进行一个机器学习项目的时候，首先应该花很少的时间快速的建立一个简单的模型，然后用交叉检验集进行测试，画出学习曲线决定我们应该增加更多的特征还是更多的样本。\n",
    "\n",
    "（3）误差分析\n",
    "\n",
    "还有一点很重要的是，要进行误差分析。手动测试在交叉验证集中误分类的样本，找到他们的共同特征和规律，由此可能会启发你如何设置新的特征，或者告诉你现在的系统有什么优点和缺点，然后想出办法来改进它。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
