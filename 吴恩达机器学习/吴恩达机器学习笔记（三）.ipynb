{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 吴恩达机器学习笔记（三）\n",
    "\n",
    "## 1、支持向量机\n",
    "\n",
    "如果了解感知机模型的话，你会知道感知机对应于输入空间中，将实例划分为正负两类的超平面。与感知机类似，支持向量机也是分类超平面。不同之处在于，感知机利用误分类最小的策略，求得分离超平面，此时有无穷多解，线性可分的支持向量机利用间隔最大化求最优分离超平面，这时，解是唯一的。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 优化目标\n",
    "\n",
    "（1）从逻辑回归开始通过一些改动来得到支持向量机\n",
    "\n",
    "在逻辑回归中，我们通常使用sigmoid函数作为假设函数，函数的形状如下\n",
    "<img style=\"float: center;\" src=\"逻辑回归的假设函数.png\" width=\"100%\"> \n",
    "\n",
    "现在，我们考虑一下逻辑回归做了什么。如果有一个$y=1$的样本，我们希望$h_{\\theta}(x)$趋近1，这就意味着，$\\theta^Tx$应该远远大于0。相反，如果有一个$y=0$的样本，我们希望$h_{\\theta}(x)$趋近0，这就意味着，$\\theta^Tx$应该远远小于0。\n",
    "\n",
    "进一步，将损失函数写出来，如下所示，对逻辑回归的损失函数做小小的改动，得到支持向量机的损失函数，\n",
    "<img style=\"float: center;\" src=\"逻辑回归与支持向量机.png\" width=\"100%\"> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、无监督学习\n",
    "\n",
    "前面学的所有机器学习算法都是监督学习，监督学习需要有标签，无监督学习不需要给数据打任何标签。对于无监督学习主要将介绍聚类算法。\n",
    "\n",
    "那么，聚类算法有什么用？下面几个应用实例：一是细分市场(market segmentation)。假设有一个客户数据库，想要将所有客户划分至不同的细分市场组，以便于营销或服务。 \n",
    "\n",
    "这在社交分析体系中，已经存在多年了。比如，观察一群人，社交网络像 Facebook、 Google+ 或者有关个人的信息，你通常和谁有电子邮件来往，他们又和谁有电子邮件来往。或者查找一群相互有联系的人。这是另一种聚类算法。你会想要在一个社交网络中查找那些其他相互有关联的朋友。\n",
    "\n",
    "还可以使用聚类来组织运算集群或组织数据中心，因为，如果你知道在集群中，哪些计算机的数据中心倾向于一起工作。你可以用它重新组织你的资源，网络的布局，数据中心和通信。 \n",
    "\n",
    "### 2.1 k-means算法\n",
    "\n",
    "输入：分类数K，训练集$\\{x^{(1)}, x^{(2)},\\ldots,x^{(m)}\\}$。\n",
    "\n",
    "输出：各个样本的分类。\n",
    "\n",
    "K-means算法的步骤可以分为不断循环的两步，簇分类和移动中心点。具体操作如下，\n",
    "\n",
    "1、（簇分类）首先随机的生成K个聚类中心点，然后计算所有样本到这K个聚类中心的距离，把该样本点分配给距离它最近的距离中心；\n",
    "\n",
    "2、（移动中心点）把当前类别样本的均值作为新的聚类中心；\n",
    "\n",
    "3、（簇分类）再对所有样本进行一次新的簇分类；\n",
    "\n",
    "4、不断重复步骤2、3，直到聚类中心不在变化。\n",
    "\n",
    "如果存在一个没有点的聚类中心怎么办？最常见的做法就是直接移除那个聚类中心，这样的话你得到的将是$K-1$个簇，而不是K个。如果确实需要有K个簇，那么就重新随机初始化这个聚类中心。\n",
    "\n",
    "### 2.2 k-means的优化目标函数\n",
    "\n",
    "知道k-means的优化目标函数能够帮我们对学习算法进行调试，确保算法实现正确，另外，我们可以利用它帮助k-means算法找到更好的簇，避免局部最优。\n",
    "\n",
    "为了更好的表示目标函数，定义，$c^{(i)}$为$x^{(i)}$所属的类别，$\\mu_k$为第k个类别的聚类中心，那么对应的$\\mu_{c^{(i)}}$就是第$c^{(i)}$个类别的聚类中心，\n",
    "\n",
    "$$J(c^{(1)},\\ldots,c^{(m)},\\mu_1,\\ldots,\\mu_K)=\\frac{1}{m}\\sum_{i=1}^m\\|x^{(i)}-\\mu_{c^{(i)}}\\|^2$$\n",
    "\n",
    "优化目标是使$J(c^{(1)},\\ldots,c^{(m)},\\mu_1,\\ldots,\\mu_K)$最小。\n",
    "\n",
    "### 2.3 随机初始化聚类中心\n",
    "\n",
    "有很多种初始化聚类中心的方法，但是有一种效果最好，就是随机的选择K个样本点作为聚类中心。由于初始化选择的点不同，得到的聚类效果可能也不同。以下图为例，对于左边的数据集，好像可以分成3个簇，实际上如果初始聚类中心选的不好，也可能得到右下方的两种完全不想要的结果。\n",
    "\n",
    "<img style=\"float: center;\" src=\"随机初始化聚类中心.png\" width=\"100%\"> \n",
    "\n",
    "那么，如何发现并且解决这个问题呢？\n",
    "\n",
    "这时候我们的目标函数就派上用场了。我们可以多次随机初始化聚类中心并求得收敛后的聚类中心，对于这些不同的结果，我们选择目标函数最小的那个。特别是K较小的情况下，多次随机初始化更适合。K很大的情况下，使用多次随机初始化也能让你找到一个相对比较好的聚类效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
